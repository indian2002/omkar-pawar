{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca333544",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "2. Is it OK to initialize the bias terms to 0?\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using an SGD optimizer?\n",
    "6. Name three ways you can produce a sparse model.\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)? What about MC Dropout?\n",
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "converging faster than before? Does it produce a better model? How does it affect\n",
    "training speed?\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb478940",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Yes, it is generally okay to initialize all the weights to the same value as long as that value is randomly selected using He initialization. He initialization scales the initial weights based on the number of input units, which helps prevent the vanishing or exploding gradients problem during training. Using the same initial value for all weights ensures that they start from a similar point, and the randomness introduced by He initialization helps break symmetry.\n",
    "\n",
    "2. Yes, it is generally okay to initialize the bias terms to 0. Unlike weights, which control the strength of connections between neurons, biases control the shift or offset of the activation function, affecting the overall output of the neuron. Initializing biases to 0 ensures that the network starts with no preference for positive or negative values, allowing it to learn more effectively during training.\n",
    "\n",
    "3. Three advantages of the SELU (Scaled Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "\n",
    "   a. **Self-normalization**: SELU has a property of self-normalization, which helps stabilize and speed up training. This property ensures that the activations and gradients remain close to mean 0 and standard deviation 1 throughout the network, reducing the risk of vanishing or exploding gradients.\n",
    "   \n",
    "   b. **Non-zero mean**: SELU has a non-zero mean, which helps alleviate the dying ReLU problem, where neurons can become inactive (outputting zero) during training. The non-zero mean ensures that neurons remain active and continue to learn even for negative inputs.\n",
    "   \n",
    "   c. **Better performance**: In some cases, SELU has been shown to outperform ReLU and its variants in deep neural networks, especially for networks with many layers. It can lead to faster convergence and better generalization performance.\n",
    "\n",
    "4. Activation function selection depends on the specific characteristics of the problem and the network architecture. Here are some guidelines for choosing activation functions:\n",
    "\n",
    "   - **SELU**: Suitable for deep neural networks due to its self-normalizing property and non-zero mean. It can help stabilize training and improve performance, especially for networks with many layers.\n",
    "   \n",
    "   - **Leaky ReLU and its variants**: Suitable for preventing dying ReLU problem by allowing a small gradient for negative inputs. They are robust choices for deep networks and can help alleviate the vanishing gradient problem.\n",
    "   \n",
    "   - **ReLU**: Widely used for its simplicity and computational efficiency. It works well for most problems but may suffer from dying ReLU problem for negative inputs.\n",
    "   \n",
    "   - **Tanh**: Suitable for hidden layers in feedforward neural networks. It squashes input values to the range [-1, 1], making it useful for models where inputs are normalized.\n",
    "   \n",
    "   - **Logistic (Sigmoid)**: Suitable for binary classification problems where outputs need to be in the range [0, 1]. It is also used in output layers of recurrent neural networks for probability estimation.\n",
    "   \n",
    "   - **Softmax**: Suitable for multi-class classification problems. It converts raw scores into probabilities, allowing the model to predict the class with the highest probability.\n",
    "\n",
    "5. If the momentum hyperparameter is set too close to 1 (e.g., 0.99999) when using an SGD optimizer, it can lead to oscillations or instability during training. Momentum accumulates past gradients to determine the direction of weight updates, and setting it too close to 1 can cause the optimizer to overshoot or oscillate around the minimum, making convergence difficult. It can also result in slow or erratic progress towards the optimal solution.\n",
    "\n",
    "6. Three ways to produce a sparse model are:\n",
    "\n",
    "   - **L1 Regularization (Lasso)**: Introduces a penalty term proportional to the absolute values of weights, encouraging sparsity by driving some weights to zero. It promotes feature selection by shrinking less important weights towards zero.\n",
    "   \n",
    "   - **Dropout**: Randomly drops units (neurons) from the neural network during training, forcing the remaining units to learn more robust features. Dropout effectively creates a sparse model by removing connections between neurons, reducing overfitting and promoting generalization.\n",
    "   \n",
    "   - **Pruning**: Iteratively removes less important weights or connections from the trained model based on their magnitude or contribution to the network's performance. Pruning helps reduce model size and computational complexity while preserving accuracy.\n",
    "\n",
    "7. Dropout can slow down training by introducing noise and requiring the model to learn redundant representations, but it helps prevent overfitting and improves generalization performance. During inference (making predictions on new instances), dropout is typically turned off, so it does not affect prediction speed. However, dropout can be applied during inference with Monte Carlo Dropout (MC Dropout) to estimate uncertainty and improve model calibration.\n",
    "\n",
    "8. Training a deep neural network on the CIFAR10 dataset:\n",
    "\n",
    "   a. Build a deep neural network with 20 hidden layers of 100 neurons each using He initialization and ELU activation function.\n",
    "   \n",
    "   b. Train the network on the CIFAR10 dataset using Nadam optimization and early stopping.\n",
    "   \n",
    "   c. Add Batch Normalization and compare learning curves to see if it converges faster and produces a better model.\n",
    "   \n",
    "   d. Replace Batch Normalization with SELU and ensure the network self-normalizes by standardizing input features and using LeCun normal initialization.\n",
    "   \n",
    "   e. Regularize the model with alpha dropout and explore using MC Dropout for better accuracy without retraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
