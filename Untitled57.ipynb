{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "2. How can we use indexing to do the same operation as unsqueeze?\n",
    "3. How do we show the actual contents of the memory used for a tensor?\n",
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n",
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "6. Implement matmul using Einstein summation.\n",
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "8. What are the three rules of Einstein summation notation? Why?\n",
    "9. What are the forward pass and backward pass of a neural network?\n",
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n",
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abc3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the answers to your questions:\n",
    "\n",
    "1. **How does `unsqueeze` help us to solve certain broadcasting problems?**\n",
    "   - `unsqueeze` adds a new dimension to a tensor, effectively reshaping it. This can be useful in broadcasting operations where tensors have different shapes but need to be compatible for element-wise operations. By adding dimensions to one or both tensors, `unsqueeze` ensures they have compatible shapes for broadcasting.\n",
    "\n",
    "2. **How can we use indexing to do the same operation as `unsqueeze`?**\n",
    "   - We can use slicing and indexing to achieve similar results as `unsqueeze`. For example, `tensor[:, None]` adds a new dimension to the tensor at the specified position, achieving the same effect as `unsqueeze`.\n",
    "\n",
    "3. **How do we show the actual contents of the memory used for a tensor?**\n",
    "   - To show the actual contents of the memory used for a tensor, you can use the `.storage()` method in PyTorch. This method returns the underlying storage of the tensor, which contains the actual data values.\n",
    "\n",
    "4. **When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix?**\n",
    "   - When adding a vector of size 3 to a matrix of size 3×3, the elements of the vector are added to each column of the matrix. This is because broadcasting extends the vector along the rows to match the shape of the matrix, and then element-wise addition is performed.\n",
    "\n",
    "5. **Do broadcasting and `expand_as` result in increased memory use? Why or why not?**\n",
    "   - No, broadcasting and `expand_as` do not result in increased memory use. They are memory-efficient operations that only create views of the original data without copying it. Broadcasting and `expand_as` allow us to perform operations on tensors with different shapes without explicitly duplicating the data.\n",
    "\n",
    "6. **Implement `matmul` using Einstein summation:**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def matmul_einsum(a, b):\n",
    "    return torch.einsum('ij,jk->ik', a, b)\n",
    "```\n",
    "\n",
    "7. **What does a repeated index letter represent on the lefthand side of `einsum`?**\n",
    "   - A repeated index letter on the lefthand side of `einsum` represents summation over that index. It indicates that the corresponding dimensions of the input tensors will be multiplied element-wise and then summed over.\n",
    "\n",
    "8. **What are the three rules of Einstein summation notation? Why?**\n",
    "   - The three rules of Einstein summation notation are:\n",
    "      1. **Repeating indices imply summation:** If an index appears more than once in an expression, it implies summation over that index.\n",
    "      2. **Matched indices:** Indices appearing in the same position in two tensors are matched for multiplication.\n",
    "      3. **Free indices:** Indices that do not appear in the output specify the dimensions of the output tensor.\n",
    "   - These rules simplify the notation for tensor operations and make it easier to express complex operations concisely.\n",
    "\n",
    "9. **What are the forward pass and backward pass of a neural network?**\n",
    "   - **Forward pass:** In the forward pass, input data is fed into the neural network, and computations are performed layer by layer until the output is obtained. Activation functions are applied to the intermediate results.\n",
    "   - **Backward pass:** In the backward pass (or backpropagation), gradients of the loss function with respect to the model parameters are computed using the chain rule of calculus. These gradients are then used to update the model parameters through optimization algorithms like gradient descent.\n",
    "\n",
    "10. **Why do we need to store some of the activations calculated for intermediate layers in the forward pass?**\n",
    "    - We need to store some of the activations calculated for intermediate layers in the forward pass because they are required during the backward pass for computing gradients. These activations are necessary for calculating the gradients of the loss function with respect to the parameters of the neural network using backpropagation.\n",
    "\n",
    "11. **What is the downside of having activations with a standard deviation too far away from 1?**\n",
    "    - The downside of having activations with a standard deviation too far away from 1 is that it can lead to the vanishing or exploding gradient problem during training. Activations with standard deviations significantly larger than 1 can cause exploding gradients, leading to unstable training. On the other hand, activations with standard deviations significantly smaller than 1 can cause vanishing gradients, hindering the convergence of the neural network during training.\n",
    "\n",
    "12. **How can weight initialization help avoid this problem?**\n",
    "    - Proper weight initialization techniques can help avoid the vanishing or exploding gradient problem by ensuring that activations are initialized to reasonable values. Techniques like Xavier/Glorot initialization or He initialization initialize weights in a way that ensures activations have standard deviations close to 1, which helps stabilize training and improve convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
