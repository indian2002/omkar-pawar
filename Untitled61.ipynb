{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc3170",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish\n",
    "\n",
    "2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "3. What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "4. What happens when you increase the size of batch computation?\n",
    "\n",
    "5. Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "6. What are loss and cost functions in deep learning?\n",
    "\n",
    "7. What do ou mean by underfitting in neural networks?\n",
    "\n",
    "8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. **Explanation of Activation Functions:**\n",
    "\n",
    "    a) **Sigmoid:** Sigmoid activation function squashes the input values between 0 and 1, which makes it useful for binary classification tasks. It has an S-shaped curve and is smooth, allowing for smooth transitions in the output. However, it suffers from the vanishing gradient problem, especially for deep neural networks.\n",
    "\n",
    "    b) **Tanh (Hyperbolic Tangent):** Tanh activation function is similar to the sigmoid function but squashes input values between -1 and 1. Like sigmoid, it is smooth and has an S-shaped curve. Tanh is often used in hidden layers of neural networks, helping to center and normalize the data.\n",
    "\n",
    "    c) **ReLU (Rectified Linear Unit):** ReLU activation function replaces negative input values with zero and leaves positive values unchanged. It introduces non-linearity to the network and helps mitigate the vanishing gradient problem. ReLU is computationally efficient and is widely used in deep learning models.\n",
    "\n",
    "    d) **ELU (Exponential Linear Unit):** ELU activation function is similar to ReLU for positive values but has a non-zero output for negative values, which helps with the dying ReLU problem. ELU can improve learning dynamics by encouraging the model to learn robust representations.\n",
    "\n",
    "    e) **LeakyReLU:** LeakyReLU activation function is an extension of ReLU that allows a small, non-zero gradient when the input is negative. This helps alleviate the dying ReLU problem and allows gradients to flow even for negative inputs.\n",
    "\n",
    "    f) **Swish:** Swish activation function is a recently proposed activation function that tends to work well in practice. It combines features of ReLU and sigmoid functions, providing non-linearity while still preserving smoothness and differentiability. Swish has been reported to outperform ReLU in certain scenarios.\n",
    "\n",
    "2. **Effect of Optimizer Learning Rate:**\n",
    "   - **Increase:** A higher learning rate can cause the optimizer to take larger steps during optimization, which may lead to faster convergence but also risks overshooting the optimal solution or instability.\n",
    "   - **Decrease:** A lower learning rate can lead to slower convergence but may help the optimizer settle into a more stable and precise minimum, especially in complex optimization landscapes.\n",
    "\n",
    "3. **Effect of Number of Internal Hidden Neurons:**\n",
    "   - **Increase:** Increasing the number of internal hidden neurons increases the model's capacity to learn complex patterns in the data. This may lead to better performance on the training data but could also increase the risk of overfitting, especially if the model becomes too complex for the given dataset.\n",
    "\n",
    "4. **Effect of Batch Size in Computation:**\n",
    "   - **Increase:** Increasing the batch size can lead to faster computation as more data is processed in each iteration. It can also provide a more stable estimate of the gradient, especially in noisy data. However, larger batch sizes may require more memory and can lead to slower convergence in some cases.\n",
    "\n",
    "5. **Purpose of Regularization to Avoid Overfitting:**\n",
    "   - Regularization techniques, such as L1 or L2 regularization, dropout, or early stopping, are used to prevent overfitting in neural networks.\n",
    "   - Overfitting occurs when the model learns to memorize the training data instead of generalizing patterns, leading to poor performance on unseen data.\n",
    "   - Regularization methods introduce constraints on the model's parameters, preventing them from becoming too large or complex, thereby promoting simpler models that generalize better.\n",
    "\n",
    "6. **Loss and Cost Functions in Deep Learning:**\n",
    "   - **Loss Function:** Measures the error between the predicted output of the model and the true target values. It quantifies how well the model is performing on individual data points.\n",
    "   - **Cost Function:** Represents the average loss over the entire training dataset. It is used to optimize the model parameters during training by minimizing the cost function through techniques like gradient descent.\n",
    "\n",
    "7. **Underfitting in Neural Networks:**\n",
    "   - Underfitting occurs when the model is too simple to capture the underlying structure of the data. It may result from using insufficiently complex models or inadequate training.\n",
    "   - Signs of underfitting include high training error and poor performance on both training and validation datasets.\n",
    "\n",
    "8. **Purpose of Dropout in Neural Networks:**\n",
    "   - Dropout is a regularization technique used to prevent overfitting in neural networks.\n",
    "   - It randomly drops (sets to zero) a fraction of neurons during training, forcing the network to learn redundant representations and preventing co-adaptation of neurons.\n",
    "   - Dropout helps improve the generalization of the model by reducing reliance on specific neurons and encourages the network to learn more robust features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
