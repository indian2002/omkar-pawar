{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n",
    "3. Name three popular activation functions. Can you draw them?\n",
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    " What is the shape of the input matrix X?\n",
    " What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    " What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    " What is the shape of the network’s output matrix Y?\n",
    " Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n",
    "\n",
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n",
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n",
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47564614",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. **Logistic Regression vs. Perceptron:**\n",
    "    - **Preferability of Logistic Regression:**\n",
    "        - Logistic Regression provides probabilistic outputs, making it more suitable for classification tasks, especially when class separation is not clear-cut.\n",
    "        - Logistic Regression can handle linearly inseparable data by using a non-linear activation function.\n",
    "        - It converges faster and is less sensitive to outliers compared to the Perceptron.\n",
    "    - **Tweaking Perceptron to Make It Equivalent to Logistic Regression:**\n",
    "        - Use a logistic (sigmoid) activation function instead of a step function in the Perceptron.\n",
    "        - Implement gradient descent with the logistic loss function (cross-entropy loss) instead of the Perceptron training algorithm.\n",
    "\n",
    "2. **Importance of Logistic Activation Function in Training MLPs:**\n",
    "    - The logistic activation function (sigmoid) was crucial in training the first MLPs because it allows the model to output probabilities, making it suitable for binary classification tasks.\n",
    "    - It introduces non-linearity into the network, enabling it to learn complex relationships between inputs and outputs.\n",
    "\n",
    "3. **Popular Activation Functions and Their Graphs:**\n",
    "    - Three popular activation functions:\n",
    "        1. Sigmoid (Logistic) Activation Function: \\( f(x) = \\frac{1}{1 + e^{-x}} \\)\n",
    "        2. ReLU (Rectified Linear Unit): \\( f(x) = \\max(0, x) \\)\n",
    "        3. Tanh (Hyperbolic Tangent) Activation Function: \\( f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\)\n",
    "\n",
    "4. **MLP Configuration:**\n",
    "    - **Shape of Input Matrix \\(X\\):** (Number of samples, Number of features)\n",
    "    - **Shape of Hidden Layer's Weight Vector \\(Wh\\):** (Number of input features, Number of neurons in hidden layer)\n",
    "    - **Shape of Hidden Layer's Bias Vector \\(bh\\):** (Number of neurons in hidden layer,)\n",
    "    - **Shape of Output Layer's Weight Vector \\(Wo\\):** (Number of neurons in hidden layer, Number of output neurons)\n",
    "    - **Shape of Output Layer's Bias Vector \\(bo\\):** (Number of output neurons,)\n",
    "    - **Shape of Network's Output Matrix \\(Y\\):** (Number of samples, Number of output neurons)\n",
    "    - **Equation for Computing Network's Output Matrix \\(Y\\):** \\( Y = \\text{ReLU}(X \\times Wh + bh) \\times Wo + bo \\)\n",
    "\n",
    "5. **Output Layer Configuration for Different Tasks:**\n",
    "    - **Email Classification (Spam/Ham):**\n",
    "        - One neuron in the output layer.\n",
    "        - Use the sigmoid activation function to output probabilities.\n",
    "    - **MNIST Classification:**\n",
    "        - Ten neurons in the output layer (one for each digit).\n",
    "        - Use the softmax activation function to output probabilities representing the likelihood of each class.\n",
    "\n",
    "6. **Backpropagation vs. Reverse-Mode Autodiff:**\n",
    "    - **Backpropagation:** A specific algorithm for efficiently computing gradients of the loss function with respect to the weights of a neural network. It works by propagating error gradients backward through the network while applying the chain rule.\n",
    "    - **Reverse-Mode Autodiff:** A more general technique for automatically computing gradients of a computational graph. It's used in frameworks like TensorFlow and PyTorch to automatically compute gradients during the training process. Backpropagation is essentially reverse-mode autodiff applied to neural networks.\n",
    "\n",
    "7. **Hyperparameters in an MLP and Overfitting Mitigation:**\n",
    "    - **Hyperparameters:**\n",
    "        - Learning rate\n",
    "        - Number of hidden layers\n",
    "        - Number of neurons in each hidden layer\n",
    "        - Activation functions\n",
    "        - Regularization parameters (e.g., L2 regularization)\n",
    "        - Dropout rate\n",
    "    - **Overfitting Mitigation:**\n",
    "        - Decrease model complexity (reduce the number of hidden layers/neurons)\n",
    "        - Increase regularization strength\n",
    "        - Use dropout\n",
    "        - Early stopping\n",
    "        - Data augmentation\n",
    "\n",
    "8. **Training a Deep MLP on MNIST:**\n",
    "    - To train a deep MLP on the MNIST dataset, one would typically use a framework like TensorFlow or PyTorch.\n",
    "    - Here's a high-level overview of steps involved:\n",
    "        - Load the MNIST dataset.\n",
    "        - Preprocess the data (normalize, reshape, etc.).\n",
    "        - Design the architecture of the MLP (number of layers, number of neurons, activation functions, etc.).\n",
    "        - Train the model using backpropagation and optimization algorithms like stochastic gradient descent (SGD) or Adam.\n",
    "        - Evaluate the model's performance on a separate validation set.\n",
    "        - Fine-tune hyperparameters to improve performance.\n",
    "        - Save checkpoints, handle interruptions, add summaries, and plot learning curves using TensorBoard for monitoring training progress.\n",
    "        - Aim for over 98% precision by tuning the model architecture and hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
