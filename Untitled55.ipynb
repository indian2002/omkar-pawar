{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What does a SavedModel contain? How do you inspect its content?\n",
    "2. When should you use TF Serving? What are its main features? What are some tools you can\n",
    "use to deploy it?\n",
    "3. How do you deploy a model across multiple TF Serving instances?\n",
    "4. When should you use the gRPC API rather than the REST API to query a model served by TF\n",
    "Serving?\n",
    "5. What are the different ways TFLite reduces a model’s size to make it run on a mobile or\n",
    "embedded device?\n",
    "6. What is quantization-aware training, and why would you need it?\n",
    "7. What are model parallelism and data parallelism? Why is the latter\n",
    "generally recommended?\n",
    "8. When training a model across multiple servers, what distribution strategies can you use?\n",
    "How do you choose which one to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. **SavedModel Contents and Inspection:**\n",
    "    - **Contents:** A SavedModel contains the TensorFlow graph definition, variables, assets, and serialization metadata necessary to restore the model.\n",
    "    - **Inspection:** You can inspect the contents of a SavedModel using TensorFlow's `saved_model_cli` command-line tool. By running `saved_model_cli show` on a SavedModel directory, you can view the signature of the model, its input and output tensors, and other metadata.\n",
    "\n",
    "2. **When to Use TF Serving and its Features:**\n",
    "    - **Use Cases:** TF Serving is used when you need to deploy TensorFlow models for serving predictions in production environments.\n",
    "    - **Main Features:**\n",
    "        - **Model Versioning:** Supports serving multiple versions of the same model concurrently.\n",
    "        - **Scalability:** TF Serving is designed for high-performance serving, capable of handling large numbers of requests concurrently.\n",
    "        - **REST and gRPC APIs:** Supports both RESTful and gRPC APIs for serving predictions.\n",
    "        - **Load Balancing:** Supports load balancing and scaling across multiple instances to distribute incoming requests.\n",
    "    - **Deployment Tools:** TF Serving can be deployed using Docker containers, Kubernetes, or directly on server instances.\n",
    "\n",
    "3. **Deploying Across Multiple TF Serving Instances:**\n",
    "    - To deploy a model across multiple TF Serving instances, you can set up a load balancer or use a service mesh like Istio to distribute incoming requests across the instances.\n",
    "    - Each TF Serving instance should have the same version of the model loaded, ensuring consistency in predictions.\n",
    "\n",
    "4. **gRPC vs REST API in TF Serving:**\n",
    "    - **gRPC API:** Should be used when low latency and high throughput are critical requirements. gRPC offers better performance compared to REST due to its binary protocol and multiplexing capabilities.\n",
    "    - **REST API:** Generally used when interoperability with existing systems or simplicity of integration is a priority. REST APIs are easier to work with in web applications and can be accessed using standard HTTP requests.\n",
    "\n",
    "5. **Reducing Model Size with TFLite:**\n",
    "    - **Quantization:** TFLite supports quantization techniques to reduce the precision of model parameters, resulting in smaller model size.\n",
    "    - **Pruning:** TFLite can prune unnecessary connections or parameters in the model, reducing its size without significant loss in performance.\n",
    "    - **Compression:** TFLite employs compression algorithms to further reduce the size of the model without sacrificing much accuracy.\n",
    "\n",
    "6. **Quantization-Aware Training:**\n",
    "    - **Definition:** Quantization-aware training is a technique used to train models with the awareness of quantization during training. It involves simulating the effects of quantization on model parameters and activations.\n",
    "    - **Need:** Quantization-aware training is necessary to ensure that the model performs well under reduced precision conditions, such as when deploying on mobile or embedded devices with limited computational resources.\n",
    "\n",
    "7. **Model Parallelism vs Data Parallelism:**\n",
    "    - **Model Parallelism:** Involves splitting the model across multiple devices or nodes, with each device responsible for computing a portion of the model.\n",
    "    - **Data Parallelism:** Involves replicating the model across multiple devices or nodes, with each device processing a different batch of data.\n",
    "    - **Recommendation:** Data parallelism is generally recommended because it is easier to implement, scales well with increasing batch sizes, and is more fault-tolerant compared to model parallelism.\n",
    "\n",
    "8. **Distribution Strategies for Training Across Multiple Servers:**\n",
    "    - **Strategies:** Common distribution strategies include MirroredStrategy, ParameterServerStrategy, and CentralStorageStrategy.\n",
    "    - **Choosing Strategy:** The choice depends on factors like model size, available computational resources, network bandwidth, and communication overhead. MirroredStrategy is often preferred for synchronous training with multiple GPUs on a single server, while ParameterServerStrategy and CentralStorageStrategy are suitable for distributed training across multiple servers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
