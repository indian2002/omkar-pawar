{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cb43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n",
    "2. Is it okay to initialize the bias terms to 0?\n",
    "3. Name three advantages of the ELU activation function over ReLU.\n",
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using a MomentumOptimizer?\n",
    "6. Name three ways you can produce a sparse model.\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. **Initializing Weights to the Same Value with He Initialization:**\n",
    "    - No, it's not generally advisable to initialize all weights to the same value, even with He initialization. The purpose of He initialization is to set initial weights to random values drawn from a specific distribution (e.g., Gaussian distribution with zero mean and variance proportional to the number of input neurons). Initializing all weights to the same value would lead to symmetry breaking issues and may hinder the learning process.\n",
    "\n",
    "2. **Initializing Bias Terms to 0:**\n",
    "    - Yes, it is generally okay to initialize bias terms to 0. Bias terms are additional parameters in neural networks that allow the model to fit the data better by shifting the activation function. Initializing bias terms to 0 is a common practice and often works well in practice.\n",
    "\n",
    "3. **Advantages of ELU Activation Function over ReLU:**\n",
    "    - Handles negative values gracefully, avoiding the dying ReLU problem.\n",
    "    - Allows the model to learn representations with more flexibility, potentially leading to better performance.\n",
    "    - Smooth gradient for all values, which can accelerate convergence during training.\n",
    "\n",
    "4. **Cases for Using Different Activation Functions:**\n",
    "    - **ELU:** Suitable as a general-purpose activation function, especially when avoiding dead neurons is important.\n",
    "    - **Leaky ReLU (and variants):** Helpful when addressing the dying ReLU problem and preventing gradient saturation.\n",
    "    - **ReLU:** Often used as a default choice due to its simplicity and effectiveness, but may suffer from dead neurons.\n",
    "    - **Tanh:** Suitable for hidden layers in feedforward neural networks when the output needs to be normalized between -1 and 1.\n",
    "    - **Logistic (Sigmoid):** Typically used in binary classification tasks as the output activation function to produce probabilities.\n",
    "    - **Softmax:** Used in the output layer of multi-class classification tasks to produce probability distributions over multiple classes.\n",
    "\n",
    "5. **Effect of Setting Momentum Hyperparameter Close to 1:**\n",
    "    - Setting the momentum hyperparameter too close to 1, such as 0.99999, in a MomentumOptimizer can cause the optimizer to have a strong memory of past gradients. This can lead to slow convergence or oscillations in the optimization process, hindering the model's ability to find the optimal solution.\n",
    "\n",
    "6. **Ways to Produce a Sparse Model:**\n",
    "    - **L1 Regularization:** Penalize large weights by adding the absolute values of weights to the loss function, encouraging sparsity.\n",
    "    - **Dropout:** Randomly set a fraction of neuron activations to zero during training, which can induce sparsity and prevent co-adaptation of neurons.\n",
    "    - **Pruning:** Remove connections or neurons with negligible contributions to the model's performance based on certain criteria (e.g., weight magnitude, sensitivity analysis).\n",
    "\n",
    "7. **Effect of Dropout on Training and Inference:**\n",
    "    - **Training:** Dropout may slow down training as it effectively reduces the capacity of the network and requires longer training times to converge. However, it often leads to better generalization and improved performance on unseen data.\n",
    "    - **Inference:** Dropout does not slow down inference significantly since it is typically turned off during inference (i.e., making predictions on new instances). During inference, the dropout layers are usually bypassed or scaled to maintain the expected activations. Therefore, inference with dropout is generally fast."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
