{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc04646",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the main tasks that autoencoders are used for?\n",
    "2. Suppose you want to train a classifier, and you have plenty of unlabeled training data but\n",
    "only a few thousand labeled instances. How can autoencoders help? How would you\n",
    "proceed?\n",
    "3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder?\n",
    "How can you evaluate the performance of an autoencoder?\n",
    "4. What are undercomplete and overcomplete autoencoders? What is the main risk of an\n",
    "excessively undercomplete autoencoder? What about the main risk of an overcomplete\n",
    "autoencoder?\n",
    "5. How do you tie weights in a stacked autoencoder? What is the point of doing so?\n",
    "6. What is a generative model? Can you name a type of generative autoencoder?\n",
    "7. What is a GAN? Can you name a few tasks where GANs can shine?\n",
    "8. What are the main difficulties when training GANs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb2213",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. **Main Tasks of Autoencoders:**\n",
    "    - **Dimensionality Reduction:** Autoencoders can be used for reducing the dimensionality of data while retaining its important features.\n",
    "    - **Data Denoising:** They can help in removing noise from data by learning to reconstruct clean inputs from noisy ones.\n",
    "    - **Feature Learning:** Autoencoders can automatically learn useful representations or features from raw data.\n",
    "    - **Anomaly Detection:** They can detect anomalies or outliers by reconstructing input data and comparing it with the original.\n",
    "    - **Data Compression:** Autoencoders can compress data into a lower-dimensional representation, useful for efficient storage and transmission.\n",
    "\n",
    "2. **Using Autoencoders for Semi-supervised Learning:**\n",
    "    - With plenty of unlabeled data and few labeled instances, autoencoders can help in learning meaningful representations of the data.\n",
    "    - Firstly, pretrain the autoencoder using the abundant unlabeled data to learn useful features or representations of the data.\n",
    "    - Then, fine-tune the classifier using the labeled instances, possibly using the learned representations as features.\n",
    "    - The learned representations from the autoencoder act as a form of regularization and can help improve generalization on the limited labeled data.\n",
    "\n",
    "3. **Evaluation of Autoencoder Performance:**\n",
    "    - Perfect reconstruction alone doesn't guarantee a good autoencoder.\n",
    "    - Other factors to consider include the model's ability to generalize to unseen data, its capacity to capture meaningful features, and its robustness to noise.\n",
    "    - Evaluation metrics can include reconstruction error, visualization of reconstructed samples, and performance on downstream tasks (e.g., classification using encoded features).\n",
    "\n",
    "4. **Undercomplete and Overcomplete Autoencoders:**\n",
    "    - **Undercomplete Autoencoders:** Have an encoding dimension smaller than the input dimension, forcing them to learn a compressed representation. The main risk is loss of information due to excessive compression, leading to poor reconstruction quality and loss of important features.\n",
    "    - **Overcomplete Autoencoders:** Have an encoding dimension larger than the input dimension, allowing them to potentially learn redundant or trivial representations. The main risk is overfitting and lack of generalization to unseen data.\n",
    "\n",
    "5. **Tying Weights in Stacked Autoencoders:**\n",
    "    - In stacked autoencoders, tying weights involves using the transpose of the weights learned in the encoding layers as the decoding layer's weights.\n",
    "    - This constraint reduces the number of parameters, prevents overfitting, and encourages the model to learn a more robust representation of the data.\n",
    "\n",
    "6. **Generative Model and Generative Autoencoder:**\n",
    "    - **Generative Model:** A model that learns the underlying probability distribution of the data and can generate new samples from that distribution.\n",
    "    - **Generative Autoencoder:** A type of autoencoder that learns to generate new data samples similar to those in the training set. Variational Autoencoder (VAE) is a type of generative autoencoder.\n",
    "\n",
    "7. **GAN (Generative Adversarial Network):**\n",
    "    - **Definition:** GAN is a type of generative model consisting of two neural networks, a generator, and a discriminator, trained adversarially.\n",
    "    - **Tasks where GANs Shine:**\n",
    "        - Image Generation: GANs are widely used for generating realistic images, such as faces, artworks, or scenes.\n",
    "        - Data Augmentation: GANs can generate synthetic data samples, which can be used for augmenting training data in various tasks, including image classification.\n",
    "        - Super-resolution: GANs can be used to enhance the resolution of images, generating high-quality images from low-resolution inputs.\n",
    "\n",
    "8. **Difficulties in Training GANs:**\n",
    "    - **Mode Collapse:** GANs may fail to explore the entire distribution of the data and instead collapse to a few modes, generating limited varieties of samples.\n",
    "    - **Training Instability:** GAN training can be unstable, with the generator and discriminator reaching an equilibrium or oscillating during training.\n",
    "    - **Gradient Vanishing/Exploding:** Gradient updates in GANs may suffer from vanishing or exploding gradients, making it difficult to train the models effectively.\n",
    "    - **Hyperparameter Sensitivity:** GAN performance is sensitive to hyperparameters, requiring careful tuning for stable training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
