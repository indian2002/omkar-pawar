{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12241369",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Write the Python code to implement a single neuron.\n",
    "2. Write the Python code to implement ReLU.\n",
    "3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions\n",
    "and functionality built into Python).\n",
    "5. What is the “hidden size” of a layer?\n",
    "6. What does the t method do in PyTorch?\n",
    "7. Why is matrix multiplication written in plain Python very slow?\n",
    "8. In matmul, why is ac==br?\n",
    "9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "10. What is elementwise arithmetic?\n",
    "11. Write the PyTorch code to test whether every element of a is greater than the\n",
    "corresponding element of b.\n",
    "12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "13. How does elementwise arithmetic help us speed up matmul?\n",
    "14. What are the broadcasting rules?\n",
    "15. What is expand_as? Show an example of how it can be used to match the results of\n",
    "broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Here are the answers to your questions:\n",
    "\n",
    "1. **Python code to implement a single neuron:**\n",
    "```python\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def activate(self, inputs):\n",
    "        z = sum(w * x for w, x in zip(self.weights, inputs)) + self.bias\n",
    "        return z\n",
    "```\n",
    "\n",
    "2. **Python code to implement ReLU:**\n",
    "```python\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "```\n",
    "\n",
    "3. **Python code for a dense layer in terms of matrix multiplication:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return np.dot(inputs, self.weights) + self.bias\n",
    "```\n",
    "\n",
    "4. **Python code for a dense layer in plain Python:**\n",
    "```python\n",
    "class DenseLayer:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return [sum(w * x for w, x in zip(self.weights, inp)) + self.bias for inp in inputs]\n",
    "```\n",
    "\n",
    "5. **Hidden size of a layer:**\n",
    "   - The hidden size of a layer refers to the number of neurons or units in that layer. It represents the dimensionality of the output space of that layer.\n",
    "\n",
    "6. **The `t` method in PyTorch:**\n",
    "   - The `t` method in PyTorch is used to transpose a tensor. It swaps the dimensions of the tensor.\n",
    "\n",
    "7. **Why matrix multiplication in plain Python is slow:**\n",
    "   - Matrix multiplication implemented in plain Python is slow because Python lists and loops are not optimized for numerical operations. It involves multiple nested loops and individual element-wise operations, resulting in poor performance compared to optimized numerical libraries like NumPy or TensorFlow.\n",
    "\n",
    "8. **In `matmul`, why is `ac == br`:**\n",
    "   - In matrix multiplication, the number of columns in the first matrix (`a`) must be equal to the number of rows in the second matrix (`b`) to perform valid matrix multiplication. Hence, `ac` must be equal to `br`.\n",
    "\n",
    "9. **Measuring time taken for a single cell execution in Jupyter Notebook:**\n",
    "   - You can use the `%timeit` magic command in Jupyter Notebook to measure the time taken for a single cell to execute. Simply write `%timeit` at the beginning of the cell followed by the code you want to measure.\n",
    "\n",
    "10. **Elementwise arithmetic:**\n",
    "    - Elementwise arithmetic refers to performing arithmetic operations (addition, subtraction, multiplication, division, etc.) independently on each element of two arrays or tensors of the same shape.\n",
    "\n",
    "11. **PyTorch code to test whether every element of a is greater than b:**\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "result = (a > b).all()\n",
    "print(result)\n",
    "```\n",
    "\n",
    "12. **Rank-0 tensor and conversion to plain Python data type:**\n",
    "    - A rank-0 tensor is a scalar, i.e., a tensor with zero dimensions.\n",
    "    - To convert it to a plain Python data type, you can use the `.item()` method in PyTorch.\n",
    "\n",
    "13. **How elementwise arithmetic helps speed up matmul:**\n",
    "    - Elementwise arithmetic allows parallelization of operations across elements, which can significantly speed up the computation of matrix multiplication, especially on hardware with parallel processing capabilities like GPUs.\n",
    "\n",
    "14. **Broadcasting rules:**\n",
    "    - Broadcasting is a set of rules that allows tensors of different shapes to be combined in elementwise operations. The smaller tensor is \"broadcasted\" across the larger tensor so that they have compatible shapes for elementwise operations. The broadcasting rules ensure that dimensions are aligned properly for the operation to be valid.\n",
    "\n",
    "15. **`expand_as` in PyTorch and example:**\n",
    "    - `expand_as` is a method in PyTorch used to expand the dimensions of a tensor to match the shape of another tensor.\n",
    "    - Example:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
    "b = torch.tensor([[5], [6]])        # Shape: (2, 1)\n",
    "\n",
    "# Broadcast tensor b to match the shape of tensor a\n",
    "expanded_b = b.expand_as(a)\n",
    "\n",
    "print(expanded_b)\n",
    "```\n",
    "This will output:\n",
    "```\n",
    "tensor([[5, 5],\n",
    "        [6, 6]])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
